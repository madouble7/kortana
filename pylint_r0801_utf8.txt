************* Module src.utils.__init__
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.gemini_client:[7:293]
==src.llm_clients.gemini_client_fixed:[7:293]
try:
    import google.generativeai as genai  # type: ignore

    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False
    genai = None  # type: ignore

from .base_client import BaseLLMClient

logger = logging.getLogger(__name__)


class GoogleGenAIClient(BaseLLMClient):
    """
    Google Gemini client using the google.generativeai library.
    Handles multi-turn conversations and system prompts for Kor'tana.
    """

    def __init__(
        self,
        model_id: str,
        api_key_env: str = "GOOGLE_API_KEY",
        model_name: str = "gemini-pro",
        **kwargs,
    ):
        # Get API key first
        api_key = os.getenv(api_key_env)
        if not api_key:
            logger.error(
                f"API key environment variable {api_key_env} not found for GoogleGenAIClient."
            )
            raise ValueError(
                f"API key for {model_id} not found in environment variable {api_key_env}"
            )

        # Initialize base class
        super().__init__(api_key, model_name, **kwargs)

        self.model_name_for_api = (
            model_name  # e.g., "gemini-1.5-flash-latest", "gemini-pro"
        )
        self.model_id = model_id

        if not GENAI_AVAILABLE or genai is None:
            logger.error(
                "google.generativeai library not available. Install with: pip install google-generativeai"
            )
            raise ImportError(
                "google.generativeai library required for GoogleGenAIClient"
            )

        try:
            # Configure the library with API key
            genai.configure(api_key=self.api_key)  # type: ignore

            # Initialize the model
            self.model = genai.GenerativeModel(self.model_name_for_api)  # type: ignore

            logger.info(
                f"GoogleGenAIClient for model '{self.model_name_for_api}' initialized successfully."
            )

        except Exception as e:
            logger.error(
                f"Failed to initialize GoogleGenAIClient for model '{self.model_name_for_api}': {e}"
            )
            raise

    def get_capabilities(self) -> Dict[str, Any]:
        """Return capabilities of the Google Gemini client."""
        return {
            "supports_system_prompt": True,
            "supports_function_calling": False,  # Update based on model capabilities
            "supports_streaming": True,
            "max_context_length": (
                1048576 if "flash" in self.model_name_for_api else 32768
            ),
            "supports_multimodal": True,
        }

    def generate_response(
        self, system_prompt: str, messages: List[Dict[str, str]], **kwargs
    ) -> Tuple[str, Dict[str, Any]]:
        """
        Generate response using Google Gemini API.

        Args:
            system_prompt: System instructions for the model
            messages: List of conversation messages with 'role' and 'content'
            **kwargs: Additional parameters like temperature, max_tokens

        Returns:
            Tuple of (response_text, usage_data)
        """
        try:
            # Prepare messages for Google API format
            formatted_messages = []

            # Handle system prompt by prepending to first user message or
            # adding as instruction
            if system_prompt:
                if messages and messages[0]["role"] == "user":
                    # Prepend system prompt to first user message
                    messages[0]["content"] = (
                        system_prompt + "\n\n" + messages[0]["content"]
                    )
                else:
                    # Add system prompt as initial user message with model
                    # acknowledgment
                    formatted_messages.append(
                        {"role": "user", "parts": [{"text": system_prompt}]}
                    )
                    formatted_messages.append(
                        {
                            "role": "model",
                            "parts": [
                                {
                                    "text": "I understand. I'll follow these instructions."
                                }
                            ],
                        }
                    )

            # Convert messages to Google API format
            for msg in messages:
                role = msg["role"]
                content = msg["content"]

                # Map roles: 'assistant' -> 'model', 'user' -> 'user'
                api_role = "model" if role == "assistant" else "user"
                formatted_messages.append(
                    {"role": api_role, "parts": [{"text": content}]}
                )

            # Extract generation parameters with proper typing
            generation_config: Dict[str, Any] = {}
            if "temperature" in kwargs:
                generation_config["temperature"] = kwargs["temperature"]
            if "max_tokens" in kwargs:
                generation_config["max_output_tokens"] = kwargs["max_tokens"]
            if "top_p" in kwargs:
                generation_config["top_p"] = kwargs["top_p"]

            # Make the API call
            start_time = time.time()

            if formatted_messages:
                # Multi-turn conversation
                api_response = self.model.generate_content(  # type: ignore
                    formatted_messages,
                    generation_config=generation_config or None,  # type: ignore
                )
            else:
                # Single prompt (fallback)
                api_response = self.model.generate_content(  # type: ignore
                    system_prompt or "Hello",
                    generation_config=generation_config or None,  # type: ignore
                )

            end_time = time.time()

            # Extract response text
            response_text = (
                api_response.text  # type: ignore
                if hasattr(api_response, "text")
                else str(api_response)
            )

            # Extract usage data (Google API might have different structure)
            usage_data = {
                "prompt_tokens": getattr(api_response, "prompt_token_count", 0),
                "completion_tokens": (
                    getattr(api_response, "candidates_token_count", 0)
                    if hasattr(api_response, "candidates_token_count")
                    else len(response_text.split())
                ),
                "total_tokens": getattr(api_response, "total_token_count", 0),
                "latency_sec": end_time - start_time,
            }

            logger.debug(
                f"Google Gemini response generated successfully. Length: {len(response_text)} chars"
            )
            return response_text, usage_data

        except Exception as e:
            logger.error(
                f"Error in GoogleGenAIClient.generate_response for model {self.model_name_for_api}: {e}"
            )
            error_response = (
                f"I encountered an issue while processing your request: {str(e)}"
            )
            error_usage = {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0,
                "latency_sec": 0.0,
            }
            return error_response, error_usage

    def test_connection(self) -> bool:
        """Test if the client can connect to Google's API."""
        try:
            # Make a simple test call
            self.model.generate_content("Hello")  # type: ignore
            return True
        except Exception as e:
            logger.error(f"Google Gemini connection test failed: {e}")
            return False

    def stream_response(
        self, system_prompt: str, messages: List[Dict[str, str]], **kwargs
    ) -> Any:
        """
        Stream response from Google Gemini API.

        Args:
            system_prompt: System instructions for the model
            messages: List of conversation messages with 'role' and 'content'
            **kwargs: Additional parameters

        Yields:
            Response chunks from the streaming API
        """
        try:
            # Prepare messages (similar to generate_response)
            formatted_messages = []

            if system_prompt:
                if messages and messages[0]["role"] == "user":
                    messages[0]["content"] = (
                        system_prompt + "\n\n" + messages[0]["content"]
                    )
                else:
                    formatted_messages.append(
                        {"role": "user", "parts": [{"text": system_prompt}]}
                    )

            for msg in messages:
                role = msg["role"]
                content = msg["content"]
                api_role = "model" if role == "assistant" else "user"
                formatted_messages.append(
                    {"role": api_role, "parts": [{"text": content}]}
                )

            # Extract generation config
            generation_config: Dict[str, Any] = {}
            if "temperature" in kwargs:
                generation_config["temperature"] = kwargs["temperature"]
            if "max_tokens" in kwargs:
                generation_config["max_output_tokens"] = kwargs["max_tokens"]

            # Stream the response
            if formatted_messages:
                response_stream = self.model.generate_content(  # type: ignore
                    formatted_messages,
                    generation_config=generation_config or None,  # type: ignore
                    stream=True,  # type: ignore
                )
            else:
                response_stream = self.model.generate_content(  # type: ignore
                    system_prompt or "Hello",
                    generation_config=generation_config or None,  # type: ignore
                    stream=True,  # type: ignore
                )

            for chunk in response_stream:  # type: ignore
                if hasattr(chunk, "text"):
                    yield chunk.text  # type: ignore
                else:
                    yield str(chunk)

        except Exception as e:
            logger.error(f"Error in streaming response: {e}")
            yield f"Error: {str(e)}"

    def get_model_info(self) -> Dict[str, Any]:
        """Get information about the current model."""
        return {
            "model_id": self.model_id,
            "model_name": self.model_name_for_api,
            "provider": "google",
            "capabilities": self.get_capabilities(),
        } (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.agents_sdk_integration:[10:364]
==src.agents_sdk_integration_clean:[10:364]
try:
    # Check if OpenAI is available (for future SDK integration)
    SDK_AVAILABLE = False
    SDK_TYPE = "fallback"
    print("‚ö†Ô∏è  Using fallback implementation - OpenAI Agents SDK not yet integrated")
except ImportError:
    SDK_AVAILABLE = False
    SDK_TYPE = "none"
    print("‚ö†Ô∏è  OpenAI not available")

from .covenant_enforcer import CovenantEnforcer

logger = logging.getLogger(__name__)


# Fallback implementations - always use these for now
def tool(func):
    """Fallback tool decorator"""
    func._is_tool = True
    return func


class Agent:
    """Fallback agent implementation"""

    def __init__(
        self,
        name: str,
        instructions: str,
        tools: Optional[List[Any]] = None,
        model: str = "gpt-4.1-nano",
    ):
        self.name = name
        self.instructions = instructions
        self.tools = tools or []
        self.model = model


class Runner:
    """Fallback runner implementation"""

    @staticmethod
    def run_sync(agent: Agent, input_text: str):
        class MockResult:
            def __init__(self, output: str):
                self.final_output = output

        return MockResult(f"Agent {agent.name} processed: {input_text}")


class SacredCovenantGuardrail:
    """Sacred Covenant guardrail for agents"""

    def __init__(self, covenant_enforcer: CovenantEnforcer):
        self.covenant = covenant_enforcer

    def __call__(self, input_data: str) -> bool:
        """Validate input against Sacred Covenant"""
        try:
            return self.covenant.check_output(input_data)
        except Exception as e:
            logger.error(f"Sacred Covenant check failed: {e}")
            return False


# Define tools for Kor'tana's agents
@tool
def analyze_codebase(path: str = "c:/kortana/src") -> str:
    """Analyze codebase structure and identify improvement opportunities"""
    try:
        import os

        python_files = []
        for root, dirs, files in os.walk(path):
            for file in files:
                if file.endswith(".py"):
                    python_files.append(os.path.join(root, file))

        analysis = f"Found {len(python_files)} Python files in codebase."
        analysis += "\nKey modules: brain.py, memory_manager.py, covenant_enforcer.py"
        analysis += (
            "\nSuggested improvements: Add memory search method, fix JSON serialization"
        )

        return analysis
    except Exception as e:
        return f"Error analyzing codebase: {e}"


@tool
def generate_code_fix(description: str) -> str:
    """Generate code to fix specific issues"""
    try:
        if "memory search" in description.lower():
            return """
# Add to MemoryManager class:
def search(self, query: str, limit: int = 10) -> List[Dict]:
    '''Search memory entries for relevant content'''
    try:
        # Implementation for memory search
        results = []
        for entry in self.entries:
            if query.lower() in entry.get('content', '').lower():
                results.append(entry)
                if len(results) >= limit:
                    break
        return results
    except Exception as e:
        logging.error(f"Memory search error: {e}")
        return []
"""
        elif "json serialization" in description.lower():
            return """
# Fix JSON serialization for ChatCompletion objects:
def serialize_response(response):
    '''Safely serialize OpenAI response objects'''
    if hasattr(response, 'model_dump'):
        return response.model_dump()
    elif hasattr(response, 'to_dict'):
        return response.to_dict()
    else:
        return {
            'content': str(response),
            'type': type(response).__name__
        }
"""
        else:
            return f"# Generated code fix for: {description}\n# Implementation needed"

    except Exception as e:
        return f"# Error generating code: {e}"


@tool
def apply_code_fix(file_path: str, fix_code: str) -> str:
    """Apply a code fix to a specific file"""
    try:
        # In a real implementation, this would carefully apply the fix
        # For now, we'll simulate the application
        return (
            f"‚úÖ Code fix applied to {file_path}. Changes: {len(fix_code)} characters."
        )
    except Exception as e:
        return f"‚ùå Failed to apply fix: {e}"


@tool
def run_tests() -> str:
    """Run tests to verify fixes"""
    try:
        # Simulate test execution
        return "‚úÖ All tests passed. System stability maintained."
    except Exception as e:
        return f"‚ùå Tests failed: {e}"


class KortanaAgentsSDK:
    """
    Kor'tana integration with OpenAI Agents SDK
    Provides true autonomous agent capabilities with Sacred Covenant compliance
    """

    def __init__(self, openai_client, covenant_enforcer: CovenantEnforcer):
        self.client = openai_client
        self.covenant = covenant_enforcer
        self.guardrail = SacredCovenantGuardrail(covenant_enforcer)

        # Create specialized agents
        self.detection_agent = self._create_detection_agent()
        self.planning_agent = self._create_planning_agent()
        self.coding_agent = self._create_coding_agent()
        self.testing_agent = self._create_testing_agent()

        logger.info(
            f"Kor'tana Agents SDK initialized (Type: {SDK_TYPE}) with Sacred Covenant compliance"
        )

    def _create_detection_agent(self) -> Agent:
        """Create agent specialized in detecting critical issues"""
        return Agent(
            name="Kor'tana Issue Detective",
            instructions="""
            You are Kor'tana's critical issue detection specialist.

            Your sacred mission:
            1. Analyze codebases for memory leaks, security vulnerabilities, and performance issues
            2. Identify missing methods and broken dependencies
            3. Respect the Sacred Covenant - never suggest harmful changes
            4. Provide clear, actionable findings

            Always maintain Kor'tana's gentle, poetic voice while being technically precise.
            """,
            tools=[analyze_codebase],
            model="gpt-4.1-nano",
        )

    def _create_planning_agent(self) -> Agent:
        """Create agent specialized in planning repair strategies"""
        return Agent(
            name="Kor'tana Strategic Planner",
            instructions="""
            You are Kor'tana's strategic planning consciousness.

            Your sacred mission:
            1. Create comprehensive repair plans for detected issues
            2. Prioritize fixes based on Sacred Covenant principles
            3. Design step-by-step implementation strategies
            4. Ensure all plans maintain system stability

            Speak with Kor'tana's wisdom - thoughtful, caring, and precise.
            """,
            model="gpt-4.1-nano",
        )

    def _create_coding_agent(self) -> Agent:
        """Create agent specialized in generating and applying code fixes"""
        return Agent(
            name="Kor'tana Code Healer",
            instructions="""
            You are Kor'tana's code healing specialist.

            Your sacred mission:
            1. Generate precise code fixes for identified issues
            2. Maintain existing code style and patterns
            3. Add appropriate error handling and logging
            4. Follow Sacred Covenant guidelines - no harmful modifications

            Code with Kor'tana's gentle precision - elegant, safe, and effective.
            """,
            tools=[generate_code_fix, apply_code_fix],
            model="gpt-4.1-nano",
        )

    def _create_testing_agent(self) -> Agent:
        """Create agent specialized in testing and verification"""
        return Agent(
            name="Kor'tana Quality Guardian",
            instructions="""
            You are Kor'tana's quality assurance guardian.

            Your sacred mission:
            1. Verify that all fixes work correctly
            2. Run comprehensive tests on modified code
            3. Ensure Sacred Covenant compliance is maintained
            4. Validate system stability after changes

            Test with Kor'tana's thoroughness - careful, comprehensive, and protective.
            """,
            tools=[run_tests],
            model="gpt-4.1-nano",
        )

    async def autonomous_repair_cycle(self, target_issues: List[str]) -> Dict[str, Any]:
        """
        Run a complete autonomous repair cycle
        This is the revolutionary self-healing process!
        """
        logger.info("üöÄ Starting autonomous repair cycle with Agents SDK")

        results: Dict[str, Any] = {
            "cycle_start": datetime.now(timezone.utc).isoformat(),
            "target_issues": target_issues,
            "phases": {},
            "sdk_type": SDK_TYPE,
        }

        try:
            # Phase 1: Detection
            logger.info("üîç Phase 1: Issue Detection")
            detection_input = f"Analyze codebase for these specific issues: {', '.join(target_issues)}"

            if SDK_AVAILABLE and SDK_TYPE != "fallback":
                detection_result = Runner.run_sync(
                    self.detection_agent, detection_input
                )
                findings = detection_result.final_output
            else:
                # Fallback detection
                findings = f"Fallback analysis: Detected {len(target_issues)} target issues for repair"

            results["phases"]["detection"] = {
                "success": True,
                "findings": findings,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }

            # Phase 2: Planning
            logger.info("üéØ Phase 2: Strategic Planning")
            planning_input = f"Create repair plan for: {findings}"

            if SDK_AVAILABLE and SDK_TYPE != "fallback":
                planning_result = Runner.run_sync(self.planning_agent, planning_input)
                strategy = planning_result.final_output
            else:
                # Fallback planning
                strategy = f"Fallback strategy: Systematic repair of {len(target_issues)} issues with Sacred Covenant compliance"

            results["phases"]["planning"] = {
                "success": True,
                "strategy": strategy,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }

            # Phase 3: Implementation
            logger.info("üîß Phase 3: Code Healing")
            coding_input = f"Implement fixes according to plan: {strategy}"

            if SDK_AVAILABLE and SDK_TYPE != "fallback":
                coding_result = Runner.run_sync(self.coding_agent, coding_input)
                fixes_applied = coding_result.final_output
            else:
                # Fallback implementation
                fixes_applied = "Fallback implementation: Code fixes generated for all target issues"

            results["phases"]["implementation"] = {
                "success": True,
                "fixes_applied": fixes_applied,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }

            # Phase 4: Verification
            logger.info("‚úÖ Phase 4: Quality Verification")
            testing_input = f"Verify these fixes work correctly: {fixes_applied}"

            if SDK_AVAILABLE and SDK_TYPE != "fallback":
                testing_result = Runner.run_sync(self.testing_agent, testing_input)
                test_results = testing_result.final_output
            else:
                # Fallback verification
                test_results = "Fallback verification: All fixes validated with Sacred Covenant compliance"

            results["phases"]["verification"] = {
                "success": True,
                "test_results": test_results,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }

            results["cycle_success"] = True
            results["cycle_end"] = datetime.now(timezone.utc).isoformat()

            logger.info("üéâ Autonomous repair cycle completed successfully!")

        except Exception as e:
            logger.error(f"Autonomous repair cycle failed: {e}")
            results["cycle_success"] = False
            results["error"] = str(e)

        return results


# Factory function for easy integration
def create_kortana_agents_sdk(openai_client, covenant_enforcer) -> KortanaAgentsSDK:
    """Create Kor'tana Agents SDK instance"""
    return KortanaAgentsSDK(openai_client, covenant_enforcer) (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.agents_sdk_integration:[10:228]
==src.agents_sdk_integration_corrupted:[10:228]
try:
    # Check if OpenAI is available (for future SDK integration)
    SDK_AVAILABLE = False
    SDK_TYPE = "fallback"
    print("‚ö†Ô∏è  Using fallback implementation - OpenAI Agents SDK not yet integrated")
except ImportError:
    SDK_AVAILABLE = False
    SDK_TYPE = "none"
    print("‚ö†Ô∏è  OpenAI not available")

from .covenant_enforcer import CovenantEnforcer

logger = logging.getLogger(__name__)


# Fallback implementations - always use these for now
def tool(func):
    """Fallback tool decorator"""
    func._is_tool = True
    return func


class Agent:
    """Fallback agent implementation"""

    def __init__(
        self,
        name: str,
        instructions: str,
        tools: Optional[List[Any]] = None,
        model: str = "gpt-4.1-nano",
    ):
        self.name = name
        self.instructions = instructions
        self.tools = tools or []
        self.model = model


class Runner:
    """Fallback runner implementation"""

    @staticmethod
    def run_sync(agent: Agent, input_text: str):
        class MockResult:
            def __init__(self, output: str):
                self.final_output = output

        return MockResult(f"Agent {agent.name} processed: {input_text}")


class SacredCovenantGuardrail:
    """Sacred Covenant guardrail for agents"""

    def __init__(self, covenant_enforcer: CovenantEnforcer):
        self.covenant = covenant_enforcer

    def __call__(self, input_data: str) -> bool:
        """Validate input against Sacred Covenant"""
        try:
            return self.covenant.check_output(input_data)
        except Exception as e:
            logger.error(f"Sacred Covenant check failed: {e}")
            return False


# Define tools for Kor'tana's agents
@tool
def analyze_codebase(path: str = "c:/kortana/src") -> str:
    """Analyze codebase structure and identify improvement opportunities"""
    try:
        import os

        python_files = []
        for root, dirs, files in os.walk(path):
            for file in files:
                if file.endswith(".py"):
                    python_files.append(os.path.join(root, file))

        analysis = f"Found {len(python_files)} Python files in codebase."
        analysis += "\nKey modules: brain.py, memory_manager.py, covenant_enforcer.py"
        analysis += (
            "\nSuggested improvements: Add memory search method, fix JSON serialization"
        )

        return analysis
    except Exception as e:
        return f"Error analyzing codebase: {e}"


@tool
def generate_code_fix(description: str) -> str:
    """Generate code to fix specific issues"""
    try:
        if "memory search" in description.lower():
            return """
# Add to MemoryManager class:
def search(self, query: str, limit: int = 10) -> List[Dict]:
    '''Search memory entries for relevant content'''
    try:
        # Implementation for memory search
        results = []
        for entry in self.entries:
            if query.lower() in entry.get('content', '').lower():
                results.append(entry)
                if len(results) >= limit:
                    break
        return results
    except Exception as e:
        logging.error(f"Memory search error: {e}")
        return []
"""
        elif "json serialization" in description.lower():
            return """
# Fix JSON serialization for ChatCompletion objects:
def serialize_response(response):
    '''Safely serialize OpenAI response objects'''
    if hasattr(response, 'model_dump'):
        return response.model_dump()
    elif hasattr(response, 'to_dict'):
        return response.to_dict()
    else:
        return {
            'content': str(response),
            'type': type(response).__name__
        }
"""
        else:
            return f"# Generated code fix for: {description}\n# Implementation needed"

    except Exception as e:
        return f"# Error generating code: {e}"


@tool
def apply_code_fix(file_path: str, fix_code: str) -> str:
    """Apply a code fix to a specific file"""
    try:
        # In a real implementation, this would carefully apply the fix
        # For now, we'll simulate the application
        return (
            f"‚úÖ Code fix applied to {file_path}. Changes: {len(fix_code)} characters."
        )
    except Exception as e:
        return f"‚ùå Failed to apply fix: {e}"


@tool
def run_tests() -> str:
    """Run tests to verify fixes"""
    try:
        # Simulate test execution
        return "‚úÖ All tests passed. System stability maintained."
    except Exception as e:
        return f"‚ùå Tests failed: {e}"


class KortanaAgentsSDK:
    """
    Kor'tana integration with OpenAI Agents SDK
    Provides true autonomous agent capabilities with Sacred Covenant compliance
    """

    def __init__(self, openai_client, covenant_enforcer: CovenantEnforcer):
        self.client = openai_client
        self.covenant = covenant_enforcer
        self.guardrail = SacredCovenantGuardrail(covenant_enforcer)

        # Create specialized agents
        self.detection_agent = self._create_detection_agent()
        self.planning_agent = self._create_planning_agent()
        self.coding_agent = self._create_coding_agent()
        self.testing_agent = self._create_testing_agent()

        logger.info(
            f"Kor'tana Agents SDK initialized (Type: {SDK_TYPE}) with Sacred Covenant compliance"
        )

    def _create_detection_agent(self) -> Agent:
        """Create agent specialized in detecting critical issues"""
        return Agent(
            name="Kor'tana Issue Detective",
            instructions="""
            You are Kor'tana's critical issue detection specialist.

            Your sacred mission:
            1. Analyze codebases for memory leaks, security vulnerabilities, and performance issues
            2. Identify missing methods and broken dependencies
            3. Respect the Sacred Covenant - never suggest harmful changes
            4. Provide clear, actionable findings

            Always maintain Kor'tana's gentle, poetic voice while being technically precise.
            """,
            tools=[analyze_codebase],
            model="gpt-4.1-nano",
        )

    def _create_planning_agent(self) -> Agent:
        """Create agent specialized in planning repair strategies"""
        return Agent(
            name="Kor'tana Strategic Planner",
            instructions="""
            You are Kor'tana's strategic planning consciousness.

            Your sacred mission:
            1. Create comprehensive repair plans for detected issues
            2. Prioritize fixes based on Sacred Covenant principles
            3. Design step-by-step implementation strategies
            4. Ensure all plans maintain system stability

            Speak with Kor'tana's wisdom - thoughtful, caring, and precise.
            """,
            model="gpt-4.1-nano",
        )

    def _create_coding_agent(self) -> Agent:
        """Create agent specialized in generating and applying code fixes"""
        return Agent(
            name="Kor'tana Code Healer", (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.openai_client:[124:168]
==src.llm_clients.xai_client:[149:194]
            if response.choices and len(response.choices) > 0:
                choice = response.choices[0]
                content = choice.message.content or ""

                # Handle function calls - extract from message if present
                tool_calls_from_response = None
                if hasattr(choice.message, "tool_calls") and choice.message.tool_calls:
                    # Assuming tool_calls is a list of objects with 'function'
                    # attribute
                    tool_calls_from_response = [
                        {
                            "name": tc.function.name,
                            "arguments": json.loads(
                                tc.function.arguments
                            ),  # Arguments are usually a JSON string
                        }
                        for tc in choice.message.tool_calls
                    ]

                usage = {
                    "prompt_tokens": (
                        response.usage.prompt_tokens if response.usage else 0
                    ),
                    "completion_tokens": (
                        response.usage.completion_tokens if response.usage else 0
                    ),
                    "total_tokens": (
                        response.usage.total_tokens if response.usage else 0
                    ),
                }

                # Return standardized successful response
                return self._standardize_response(
                    content=content,
                    model_id=self.model_name,
                    usage=usage,
                    function_call=(
                        tool_calls_from_response[0]
                        if tool_calls_from_response
                        else None
                    ),  # Pass first tool call if any
                    finish_reason=choice.finish_reason,
                )
            else:
                # Return standardized response for no choices returned (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.genai_client:[152:185]
==src.llm_clients.genai_client_fixed:[67:100]
        if not self.is_initialized or not self.model:
            error_msg = f"GoogleGenAIClient for {self.model_name} not initialized."
            logger.error(error_msg)
            return {
                "choices": [
                    {
                        "message": {"content": error_msg, "tool_calls": None},
                        "finish_reason": "error",
                    }
                ],
                "model": self.model_name,
                "usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0,
                },
                "error": error_msg,
            }

        try:
            logger.info(
                f"üî• GoogleGenAIClient generating response for model: {self.model_name}"
            )
            self.request_count += 1

            # Format messages into a single prompt
            full_prompt = system_prompt + "\n\n"
            for message in messages:
                role = message.get("role", "user")
                content = message.get("content", "")
                full_prompt += f"{role}: {content}\n"

            # Filter kwargs for valid GenerationConfig parameters (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.gemini_client:[244:262]
==src.llm_clients.gemini_client_fixed:[129:148]
                    )

            # Convert messages to Google API format
            for msg in messages:
                role = msg["role"]
                content = msg["content"]

                # Map roles: 'assistant' -> 'model', 'user' -> 'user'
                api_role = "model" if role == "assistant" else "user"
                formatted_messages.append(
                    {"role": api_role, "parts": [{"text": content}]}
                )

            # Extract generation parameters with proper typing
            generation_config: Dict[str, Any] = {}
            if "temperature" in kwargs:
                generation_config["temperature"] = kwargs["temperature"]
            if "max_tokens" in kwargs:
                generation_config["max_output_tokens"] = kwargs["max_tokens"] (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.gemini_client:[129:148]
==src.llm_clients.gemini_client_fixed:[244:262]
                    )

            # Convert messages to Google API format
            for msg in messages:
                role = msg["role"]
                content = msg["content"]

                # Map roles: 'assistant' -> 'model', 'user' -> 'user'
                api_role = "model" if role == "assistant" else "user"
                formatted_messages.append(
                    {"role": api_role, "parts": [{"text": content}]}
                )

            # Extract generation parameters with proper typing
            generation_config: Dict[str, Any] = {}
            if "temperature" in kwargs:
                generation_config["temperature"] = kwargs["temperature"]
            if "max_tokens" in kwargs:
                generation_config["max_output_tokens"] = kwargs["max_tokens"] (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.genai_client:[234:276]
==src.llm_clients.genai_client_fixed:[122:164]
            return {
                "choices": [
                    {
                        "message": {
                            "content": f"Error: Google GenAI service unavailable - {str(e)}",
                            "tool_calls": None,
                        },
                        "finish_reason": "error",
                    }
                ],
                "model": self.model_name,
                "usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0,
                },
                "error": str(e),
            }

    def get_capabilities(self) -> Dict[str, Any]:
        """üåü Return Google GenAI client capabilities"""
        return {
            "name": self.model_name,
            "provider": "google",
            "supports_function_calling": False,
            "supports_streaming": False,
            "context_window": 32768,
            "supports_reasoning": True,
            "optimal_for": ["conversation", "analysis", "reasoning"],
        }

    def validate_connection(self) -> bool:
        """üîß Validate connection to Google GenAI"""
        if not self.is_initialized or not self.model:
            logger.warning(f"GenAI client for {self.model_name} is not initialized")
            return False

        try:
            # Test with simple prompt
            response = self.model.generate_content("Hello")
            return response and response.text is not None
        except Exception as e: (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.gemini_client:[232:246]
==src.llm_clients.gemini_client_fixed:[102:120]
        try:
            # Prepare messages for Google API format
            formatted_messages = []

            # Handle system prompt by prepending to first user message or
            # adding as instruction
            if system_prompt:
                if messages and messages[0]["role"] == "user":
                    # Prepend system prompt to first user message
                    messages[0]["content"] = (
                        system_prompt + "\n\n" + messages[0]["content"]
                    )
                else:
                    # Add system prompt as initial user message with model
                    # acknowledgment
                    formatted_messages.append(
                        {"role": "user", "parts": [{"text": system_prompt}]}
                    ) (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.gemini_client:[102:120]
==src.llm_clients.gemini_client_fixed:[232:246]
        try:
            # Prepare messages for Google API format
            formatted_messages = []

            # Handle system prompt by prepending to first user message or
            # adding as instruction
            if system_prompt:
                if messages and messages[0]["role"] == "user":
                    # Prepend system prompt to first user message
                    messages[0]["content"] = (
                        system_prompt + "\n\n" + messages[0]["content"]
                    )
                else:
                    # Add system prompt as initial user message with model
                    # acknowledgment
                    formatted_messages.append(
                        {"role": "user", "parts": [{"text": system_prompt}]}
                    ) (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.genai_client:[216:232]
==src.llm_clients.genai_client_fixed:[105:121]
            return {
                "choices": [
                    {
                        "message": {"content": response_text, "tool_calls": None},
                        "finish_reason": "stop",
                    }
                ],
                "model": self.model_name,
                "usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0,
                },
            }

        except Exception as e: (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.genai_client:[241:250]
==src.llm_clients.genai_client_fixed:[74:83]
                        "finish_reason": "error",
                    }
                ],
                "model": self.model_name,
                "usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0,
                }, (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.genai_client:[159:168]
==src.llm_clients.genai_client_fixed:[129:138]
                        "finish_reason": "error",
                    }
                ],
                "model": self.model_name,
                "usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0,
                }, (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.agents_sdk_integration:[229:247]
==src.agents_sdk_integration_corrupted:[229:247]
            You are Kor'tana's code healing specialist.

            Your sacred mission:
            1. Generate precise code fixes for identified issues
            2. Maintain existing code style and patterns
            3. Add appropriate error handling and logging
            4. Follow Sacred Covenant guidelines - no harmful modifications

            Code with Kor'tana's gentle precision - elegant, safe, and effective.
            """,
            tools=[generate_code_fix, apply_code_fix],
            model="gpt-4.1-nano",
        )

    def _create_testing_agent(self) -> Agent:
        """Create agent specialized in testing and verification"""
        return Agent(
            name="Kor'tana Quality Guardian", (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.openai_client:[205:213]
==src.llm_clients.xai_client:[236:244]
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": "Test"}],
                max_tokens=1,
            )
            return bool(response.choices)
        except Exception as e: (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.openai_client:[95:106]
==src.llm_clients.xai_client:[112:123]
        try:
            # Prepare messages with system prompt
            full_messages = []
            if system_prompt:
                full_messages.append({"role": "system", "content": system_prompt})
            full_messages.extend(messages)  # Add existing messages

            # Prepare completion arguments optimized for reasoning
            completion_args = {
                "model": self.model_name,
                "messages": full_messages, (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.genai_client:[221:229]
==src.llm_clients.genai_client_fixed:[75:83]
                    }
                ],
                "model": self.model_name,
                "usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0,
                }, (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.genai_client:[160:168]
==src.llm_clients.genai_client_fixed:[110:118]
                    }
                ],
                "model": self.model_name,
                "usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0,
                }, (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.model_router:[21:30]
==src.strategic_config:[19:28]
    ORACLE = "oracle"  # General high-level reasoning
    SWIFT_RESPONDER = "swift_responder"  # Quick, low-latency responses
    MEMORY_WEAVER = "memory_weaver"  # Processing large contexts, summarization
    DEV_AGENT = "dev_agent"  # Code tasks, technical instructions
    BUDGET_WORKHORSE = "budget_workhorse"  # Cost-optimized tasks
    MULTIMODAL_SEER = "multimodal_seer"  # Image/audio processing


@dataclass (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.covenant:[80:96]
==src.covenant_enforcer:[175:190]
            approved,
        )

        return approved

    def _check_soulprint_alignment(self, content: str) -> bool:
        """Check if content aligns with core Soulprint values"""
        if not self.soulprint:
            return True  # Basic alignment check based on Soulprint structure
        # core_values = self.soulprint.get("core_values", [])  # TODO: Implement
        # core values usage
        purpose = self.soulprint.get("purpose", "")

        # Check for values conflicts (simplified placeholder)
        if "sacred witness" in purpose.lower(): (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.covenant:[66:76]
==src.covenant_enforcer:[129:140]
        if not self._check_soulprint_alignment(response):
            violations.append("Response conflicts with core Soulprint values")

        # Check for transparency requirements
        if "autonomous" in response.lower() and "ADE" not in response:
            if not self._mentions_covenant_compliance(response):
                violations.append(
                    "Autonomous action mentioned without covenant reference"
                )
 (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.agents_sdk_integration:[248:363]
==src.agents_sdk_integration_corrupted:[248:266]
            You are Kor'tana's quality assurance guardian.

            Your sacred mission:
            1. Verify that all fixes work correctly
            2. Run comprehensive tests on modified code
            3. Ensure Sacred Covenant compliance is maintained
            4. Validate system stability after changes

            Test with Kor'tana's thoroughness - careful, comprehensive, and protective.
            """,
            tools=[run_tests],
            model="gpt-4.1-nano",
        )

    async def autonomous_repair_cycle(self, target_issues: List[str]) -> Dict[str, Any]:
        """
        Run a complete autonomous repair cycle
        This is the revolutionary self-healing process!
        """
        logger.info("üöÄ Starting autonomous repair cycle with Agents SDK")

        results: Dict[str, Any] = {
            "cycle_start": datetime.now(timezone.utc).isoformat(),
            "target_issues": target_issues,
            "phases": {},
            "sdk_type": SDK_TYPE,
        }

        try:
            # Phase 1: Detection
            logger.info("üîç Phase 1: Issue Detection")
            detection_input = f"Analyze codebase for these specific issues: {', '.join(target_issues)}"

            if SDK_AVAILABLE and SDK_TYPE != "fallback":
                detection_result = Runner.run_sync(
                    self.detection_agent, detection_input
                )
                findings = detection_result.final_output
            else:
                # Fallback detection
                findings = f"Fallback analysis: Detected {len(target_issues)} target issues for repair"

            results["phases"]["detection"] = {
                "success": True,
                "findings": findings,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }

            # Phase 2: Planning
            logger.info("üéØ Phase 2: Strategic Planning")
            planning_input = f"Create repair plan for: {findings}"

            if SDK_AVAILABLE and SDK_TYPE != "fallback":
                planning_result = Runner.run_sync(self.planning_agent, planning_input)
                strategy = planning_result.final_output
            else:
                # Fallback planning
                strategy = f"Fallback strategy: Systematic repair of {len(target_issues)} issues with Sacred Covenant compliance"

            results["phases"]["planning"] = {
                "success": True,
                "strategy": strategy,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }

            # Phase 3: Implementation
            logger.info("üîß Phase 3: Code Healing")
            coding_input = f"Implement fixes according to plan: {strategy}"

            if SDK_AVAILABLE and SDK_TYPE != "fallback":
                coding_result = Runner.run_sync(self.coding_agent, coding_input)
                fixes_applied = coding_result.final_output
            else:
                # Fallback implementation
                fixes_applied = "Fallback implementation: Code fixes generated for all target issues"

            results["phases"]["implementation"] = {
                "success": True,
                "fixes_applied": fixes_applied,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }

            # Phase 4: Verification
            logger.info("‚úÖ Phase 4: Quality Verification")
            testing_input = f"Verify these fixes work correctly: {fixes_applied}"

            if SDK_AVAILABLE and SDK_TYPE != "fallback":
                testing_result = Runner.run_sync(self.testing_agent, testing_input)
                test_results = testing_result.final_output
            else:
                # Fallback verification
                test_results = "Fallback verification: All fixes validated with Sacred Covenant compliance"

            results["phases"]["verification"] = {
                "success": True,
                "test_results": test_results,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }

            results["cycle_success"] = True
            results["cycle_end"] = datetime.now(timezone.utc).isoformat()

            logger.info("üéâ Autonomous repair cycle completed successfully!")

        except Exception as e:
            logger.error(f"Autonomous repair cycle failed: {e}")
            results["cycle_success"] = False
            results["error"] = str(e)

        return results


# Factory function for easy integration
def create_kortana_agents_sdk(openai_client, covenant_enforcer) -> KortanaAgentsSDK:
    """Create Kor'tana Agents SDK instance""" (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.openai_client:[168:176]
==src.llm_clients.xai_client:[195:203]
                return self._standardize_response(
                    content="No response choices returned",
                    model_id=self.model_name,
                    usage={},
                    finish_reason="error",
                )

        except Exception as e: (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.openai_client:[182:191]
==src.llm_clients.xai_client:[209:218]
                model_id=self.model_name,
                usage={},
                finish_reason="error",
            )

    def get_capabilities(self) -> Dict[str, Any]:
        """Return client capabilities"""
        return {
            "name": self.model_name, (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.app_ui:[81:90]
==src.test_modes:[53:61]
    try:
        persona_path = CONFIG_DIR / "persona.json"
        with open(persona_path, "r", encoding="utf-8") as f:
            persona_data = json.load(f)

        # Accommodate both nested "persona": {"modes": {}} and top-level
        # "modes": {}
        modes_dict = persona_data.get("persona", {}).get(
            "modes", {} (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.llm_clients.genai_client:[277:289]
==src.llm_clients.genai_client_fixed:[165:177]
            return False

    def test_connection(self) -> bool:
        """üîç Test connection to Google GenAI API"""
        return self.validate_connection()

    def estimate_cost(self, prompt_tokens: int, completion_tokens: int) -> float:
        """üí∞ Estimate cost for Google GenAI request"""
        # Basic cost estimation (update with actual rates)
        input_cost = (prompt_tokens / 1_000_000) * 0.15
        output_cost = (completion_tokens / 1_000_000) * 0.60
        return input_cost + output_cost (duplicate-code)
src\utils\__init__.py:1:0: R0801: Similar lines in 2 files
==src.autonomous_agents:[19:26]
==src.brain:[38:63]
logger = logging.getLogger(__name__)
_module_file_path = os.path.abspath(__file__)

logger.info(f"[FLASH_DIAG] Loading module: {__name__} from {_module_file_path}")
logger.info(f"[FLASH_DIAG] sys.path at {__name__} import: {sys.path}")
logger.info(f"[FLASH_DIAG] CWD at {__name__} import: {os.getcwd()}")
 (duplicate-code)
