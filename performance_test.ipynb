{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55531c14",
   "metadata": {},
   "source": [
    "# Environment Setup and Dependencies\n",
    "Install necessary packages such as `pytest`, `pytest-json-report`, and `pandas` to facilitate test execution and data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794468dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section: Environment Setup and Dependencies\n",
    "import sys\n",
    "!{sys.executable} -m pip install pytest pytest-json-report pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c82a3b7",
   "metadata": {},
   "source": [
    "# Execute Test Suite with Performance Instrumentation\n",
    "Run the command `!pytest tests/ --json-report --json-report-file=results.json` to execute the full suite and capture detailed timing data for the new architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c6a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set PYTHONPATH to include src\n",
    "os.environ[\"PYTHONPATH\"] = os.path.abspath(\"src\")\n",
    "\n",
    "# Run pytest on the full suite\n",
    "!{sys.executable} -m pytest tests/ --json-report --json-report-file=results.json --maxfail=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac3ac8a",
   "metadata": {},
   "source": [
    "# Load and Process Test Result Metadata\n",
    "Parse the generated JSON report to extract individual test durations $t_i$ and status for comparative analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    with open('results.json') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    tests = []\n",
    "    for test in data['tests']:\n",
    "        tests.append({\n",
    "            'nodeid': test['nodeid'],\n",
    "            'outcome': test['outcome'],\n",
    "            'duration': test['setup'].get('duration', 0) + test['call'].get('duration', 0) + test['teardown'].get('duration', 0)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(tests)\n",
    "    print(df.head())\n",
    "    print(f\"\\nTotal tests run: {len(df)}\")\n",
    "    print(f\"Passed: {len(df[df.outcome == 'passed'])}\")\n",
    "    print(f\"Failed: {len(df[df.outcome == 'failed'])}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"results.json not found. Run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31421f18",
   "metadata": {},
   "source": [
    "# Calculate Performance Variance Metrics\n",
    "Identify outliers and compute execution time statistics. $t_{mean} = \\frac{1}{n} \\sum t_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d439456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\n",
    "    print(f\"Mean duration: {df.duration.mean():.4f}s\")\n",
    "    print(f\"Max duration: {df.duration.max():.4f}s\")\n",
    "    print(f\"Min duration: {df.duration.min():.4f}s\")\n",
    "\n",
    "    # Identify slow tests (> 1.0s)\n",
    "    slow_tests = df[df.duration > 1.0].sort_values(by='duration', ascending=False)\n",
    "    if not slow_tests.empty:\n",
    "        print(\"\\nSlowest tests:\")\n",
    "        print(slow_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e0f50f",
   "metadata": {},
   "source": [
    "# Visualize Execution Time Distribution\n",
    "Generate histograms and box plots using `matplotlib` to verify that the performance of the new architecture aligns with expected baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fa8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'df' in locals():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df.duration, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "    plt.title('Distribution of Test Durations')\n",
    "    plt.xlabel('Duration (seconds)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.axvline(df.duration.mean(), color='red', linestyle='dashed', linewidth=1, label=f'Mean: {df.duration.mean():.4f}s')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
